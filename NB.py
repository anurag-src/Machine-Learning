# -*- coding: utf-8 -*-
"""Copy of Assignment_1_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18tljbzMzK4wJMhJcmdPNDMf6lKaYfR79
"""
import string
import random

f = open("dataset_NB.txt", "r")
f = f.read().split("\n")
data_global = []
val_global = []
for sent in f:
    l = len(sent)
    data_global.append(sent[:l - 1].strip())
    val_global.append(int(sent[l - 1]))

tmp = []
stopwords = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "down", "in", "out", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
for sentence in data_global:
    # Lowercase
    sentence = sentence.lower()
    # Puntuation removal
    sentence = "".join([char for char in sentence if char not in string.punctuation])
    # Tokenization
    sentence_tmp = sentence.split()
    sentence_tmp = [char for char in sentence_tmp if char not in stopwords]
    tmp.append(sentence_tmp)

data_global = tmp


def classify_nb(test_data, train_data, train_val, alpha):
    p_pos,p_neg,ppt,npt = preprocess(train_data, train_val, alpha)

    predicted = list()

    for test in test_data:

        pos_prob = p_pos
        neg_prob = p_neg

        for word in test:
            if word in ppt:
                pos_prob *= ppt[word]

            if word in npt:
                neg_prob *= npt[word]

        if neg_prob > pos_prob:
            predicted.append(0)
        else:
            predicted.append(1)

    return predicted


def preprocess(data, val, alpha):
    vocab = set()
    total_positive_words = 0
    positive_count = 0
    total_negative_words = 0
    negative_count = 0
    positive = dict()
    negative = dict()
    for j in range (0,len(data)):
        tmp = data[j]
        if val[j] == 1:
            positive_count += 1
            total_positive_words += len(tmp)
            for word in tmp:
                vocab.add(word)
                if word in positive:
                    positive[word] += 1
                else:
                    positive[word] = 1

        if val[j] == 0:
            negative_count += 1
            total_negative_words += len(tmp)
            for word in tmp:
                vocab.add(word)
                if word in negative:
                    negative[word] += 1
                else:
                    negative[word] = 1

    vocab_size = len(vocab)
    p_positive = positive_count/len(data)
    p_negative = negative_count/len(data)

    positive_probability_table = dict()
    negative_probability_table = dict()

    for key in vocab:
        if key not in positive:
            positive[key] = 0
        if key not in negative:
            negative[key] = 0

        positive_probability_table[key] = (positive[key] + alpha) / (total_positive_words + alpha * vocab_size)
        negative_probability_table[key] = (negative[key] + alpha) / (total_negative_words + alpha * vocab_size)

    return p_positive,p_negative,positive_probability_table,negative_probability_table


if __name__ == '__main__':

    k_fold = 7

    temp = list(zip(data_global, val_global))
    random.Random().shuffle(temp)
    data_global, val_global = zip(*temp)

    alpha = 1

    k_len = int(len(val_global) / k_fold)

    avg_accuracy: float = 0.0

    for i in range(0, k_fold):
        test_data = data_global[0:k_len]
        train_data = data_global[k_len:]
        test_val = val_global[0:k_len]
        train_val = val_global[k_len:]

        data_global = train_data + test_data
        val_global = train_val + test_data

        local_accuracy = 0

        predicted_val = classify_nb(test_data, train_data, train_val, alpha)

        for j in range(0, len(predicted_val)):
            if predicted_val[j] == test_val[j]:
                local_accuracy += 1

        local_accuracy = local_accuracy / (len(predicted_val))

        print("Accuracy for " + str(i + 1) + "th iteration is " + str(local_accuracy))
        avg_accuracy += local_accuracy

    avg_accuracy = avg_accuracy / k_fold
    print("Average Accuracy is " + str(avg_accuracy))

